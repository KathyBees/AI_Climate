{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldOn_BV2GwKP",
    "outputId": "865ec8f2-4e19-4602-a406-b49e5c267e75",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# if necessary, install NeuralGCM and dependencies\n",
    "!pip install -q -U neuralgcm dinosaur-dycore gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_web7Ad1gunN"
   },
   "source": [
    "# Forecasting quick start\n",
    "\n",
    "This notebook uses ERA5 data and pretrained NeuralGCM model to make a weather forecast.\n",
    "\n",
    "The forecast is made in 3 steps:\n",
    "1. Slice of ERA5 data is regridded to model resolution\n",
    "2. NeuralGCM model state is initialized and rolled out\n",
    "3. Predictions and reference trajectory are combined for visualization\n",
    "\n",
    "By default the notebook uses intermediate deterministic NeuralGCM 1.4° model. Other available checkpoints include deterministic 0.7°, 2.8° and stochastic 1.4° NeuralGCM variations.\n",
    "\n",
    "```{tip}\n",
    "You can run this notebook yourself in [Google Colab](https://colab.research.google.com/github/google-research/neuralgcm/blob/main/docs/inference_demo.ipynb). We recommend using a GPU or TPU runtime due to high memory and compute requirements.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTapB9c0AWMJ"
   },
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import jax\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xarray\n",
    "\n",
    "from dinosaur import horizontal_interpolation\n",
    "from dinosaur import spherical_harmonic\n",
    "from dinosaur import xarray_utils\n",
    "import neuralgcm\n",
    "\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the SST pattern that was send to us https://esgf-ui.ceda.ac.uk/cog/search/cmip6-ceda/ \n",
    "How to: \\\n",
    "tos in die Suchleiste\n",
    "<span style=\"color:red\">~~Download erledigt~~</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xarray netCDF4 numpy\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "ersst_path = \"/data/tos_Omon_GISS-E2-1-G_historical_r1i1p5f1_gn_200101-201412.nc\"\n",
    "ersst_data = xr.open_dataset(ersst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data to be similar to NeuralGCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5uFP46Obo80"
   },
   "source": [
    "## Load a pre-trained NeuralGCM model\n",
    "\n",
    "```{caution}\n",
    "Trained model weights are licensed for non-commercial use, under the Creative Commons [Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/) license (CC BY-NC-SA 4.0).\n",
    "```\n",
    "\n",
    "Pre-trained model checkpoints from the NeuralGCM paper are [available for download](https://console.cloud.google.com/storage/browser/gresearch/neuralgcm/04_30_2024) on Google Cloud Storage:\n",
    "\n",
    "- Deterministic models:\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_0_7_deg.pkl`\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_1_4_deg.pkl`\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_2_8_deg.pkl`\n",
    "- Stochastic models:\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_stochastic_1_4_deg.pkl`\n",
    "\n",
    "<span style=\"color:green\">NEW</span> \n",
    "## Need to train it on our own using the inputs from era5\n",
    "\n",
    "Checklist: \\\n",
    "~~Find out all variables~~ \\\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_variables = [\n",
    "    \"precipitation\",      # To track precipitation deficits\n",
    "    \"evaporation\",        # Water loss through evaporation\n",
    "    \"soil_moisture\",      # Soil water content for agricultural impacts\n",
    "    \"temperature\",        # High temperatures linked to drought\n",
    "    \"specific_humidity\",  # Tracks atmospheric moisture\n",
    "    \"surface_pressure\",   # Indicator of regional pressure systems\n",
    "    \"sea_surface_temp\",   # SST anomalies linked to teleconnections\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQnv1GWKD1iP"
   },
   "outputs": [],
   "source": [
    "model_name = 'neural_gcm_dynamic_forcing_deterministic_1_4_deg.pkl'  #@param ['neural_gcm_dynamic_forcing_deterministic_0_7_deg.pkl', 'neural_gcm_dynamic_forcing_deterministic_1_4_deg.pkl', 'neural_gcm_dynamic_forcing_deterministic_2_8_deg.pkl', 'neural_gcm_dynamic_forcing_stochastic_1_4_deg.pkl'] {type: \"string\"}\n",
    "\n",
    "with gcs.open(f'gs://gresearch/neuralgcm/04_30_2024/{model_name}', 'rb') as f:\n",
    "  ckpt = pickle.load(f)\n",
    "\n",
    "model = neuralgcm.PressureLevelModel.from_checkpoint(ckpt)\n",
    "\n",
    "##\n",
    "# TODO: Train on our own, Decide on training and testing data and do the 30 year roll out\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpEb_avqbo80"
   },
   "source": [
    "## Load ERA5 data from GCP/Zarr\n",
    "\n",
    "See {doc}`datasets` for details. Leave this part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66ZyTazL6GF7"
   },
   "source": [
    "Select out a few days of data:\n",
    "\n",
    "## Need to change start time and end time\n",
    "\n",
    "Checklist: \\\n",
    "~~Select spain as a region~~ \\\n",
    "<span style=\"color:red\">Decide on start and end time</span> \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_path = 'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
    "full_era5 = xarray.open_zarr(gcs.get_mapper(era5_path), chunks=None)\n",
    "\n",
    "demo_start_time = '2020-02-14'\n",
    "demo_end_time = '2020-02-18'\n",
    "data_inner_steps = 24  # process every 24th hour\n",
    "\n",
    "# Define the latitude and longitude bounds for Spain\n",
    "# Die Werte müssen eigentlich größer sein, da im Mittelmeer auch Abläufe sind, die das beeinflussen. Für die Fläche \n",
    "\n",
    "lat_bounds = slice(36, 44)  # Latitude range for Spain mainland\n",
    "lon_bounds = slice(-10, 4)  # Longitude range for Spain mainland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span>Subset the data before adding it to era5 input for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subset the data\n",
    "sst_subset = ersst_data['sst'].sel(time=slice(start_date, end_date),\n",
    "                                   lat=lat_bounds,\n",
    "                                   lon=lon_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span> Compute anomalies, trends, or indices such as the Atlantic Multidecadal Oscillation (AMO) to understand SST variations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the climatology (mean over the period)\n",
    "climatology = sst_subset.groupby('time.month').mean('time')\n",
    "\n",
    "# Compute SST anomalies\n",
    "sst_anomalies = sst_subset.groupby('time.month') - climatology\n",
    "\n",
    "# Calculate the AMO index (example)\n",
    "amo_index = sst_anomalies.mean(dim=['lat', 'lon'])\n",
    "\n",
    "print(climatology)\n",
    "print(sst_anomalies)\n",
    "print(amo_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot them to have something to see? Low Priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span>Incorporate the processed SST data or derived indices into your drought prediction model as predictors or covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_data = era5_data['precipitation'] #idk if it is precipitation TODO: print variable names to know\n",
    "temperature_data = era5_data['temperature']\n",
    "\n",
    "model_inputs = {\n",
    "    'precipitation': precipitation_data,\n",
    "    'temperature': temperature_data,\n",
    "    'sst_anomalies': sst_anomalies,\n",
    "    # TODO: Add other variables as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span> Add SST anomalies to the input variables of the NeuralGCM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Dbth-nDjM5F"
   },
   "outputs": [],
   "source": [
    "# Add SST anomalies to the input variables of the NeuralGCM model\n",
    "era5_with_sst = xr.Dataset(\n",
    "    {\n",
    "        'precipitation': era5_data['precipitation'],\n",
    "        'temperature': era5_data['temperature'],\n",
    "        'humidity': era5_data['specific_humidity'],\n",
    "        'sst_anomalies': sst_anomalies_grid,\n",
    "        # TODO: ADD remining inputs\n",
    "    },\n",
    "    coords={\n",
    "        'time': era5_data['time'],\n",
    "        'latitude': era5_data['latitude'],\n",
    "        'longitude': era5_data['longitude'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivRFAQnt6KKF"
   },
   "source": [
    "Regrid to NeuralGCM's native resolution: <span style=\"color:red\">Rewrite it to use the sst data</span>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62wVlyCsJ-Jg"
   },
   "outputs": [],
   "source": [
    "era5_grid = spherical_harmonic.Grid(\n",
    "    latitude_nodes=full_era5.sizes['latitude'],\n",
    "    longitude_nodes=full_era5.sizes['longitude'],\n",
    "    latitude_spacing=xarray_utils.infer_latitude_spacing(full_era5.latitude),\n",
    "    longitude_offset=xarray_utils.infer_longitude_offset(full_era5.longitude),\n",
    ")\n",
    "regridder = horizontal_interpolation.ConservativeRegridder(\n",
    "    era5_grid, model.data_coords.horizontal, skipna=True\n",
    ")\n",
    "eval_era5 = xarray_utils.regrid(sliced_era5, regridder)\n",
    "eval_era5 = xarray_utils.fill_nan_with_nearest(eval_era5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span>:Ensure that the combined dataset adheres to NeuralGCM’s expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = {\n",
    "    variable: era5_with_sst[variable].values for variable in era5_with_sst.data_vars\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrbru3K1bo81"
   },
   "source": [
    "## Make the forecast\n",
    "\n",
    "See {doc}`trained_models` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVCC2pO8eZE0"
   },
   "outputs": [],
   "source": [
    "inner_steps = 24  # save model outputs once every 24 hours\n",
    "outer_steps = 4 * 24 // inner_steps  # total of 4 days\n",
    "timedelta = np.timedelta64(1, 'h') * inner_steps\n",
    "times = (np.arange(outer_steps) * inner_steps)  # time axis in hours\n",
    "\n",
    "# initialize model state\n",
    "# TODO: change the input with the new one\n",
    "\n",
    "\n",
    "inputs = model.inputs_from_xarray(eval_era5.isel(time=0))\n",
    "input_forcings = model.forcings_from_xarray(eval_era5.isel(time=0))\n",
    "rng_key = jax.random.key(42)  # optional for deterministic models\n",
    "initial_state = model.encode(inputs, input_forcings, rng_key)\n",
    "\n",
    "# use persistence for forcing variables (SST and sea ice cover)\n",
    "all_forcings = model.forcings_from_xarray(eval_era5.head(time=1))\n",
    "\n",
    "# make forecast\n",
    "final_state, predictions = model.unroll(\n",
    "    initial_state,\n",
    "    all_forcings,\n",
    "    steps=outer_steps,\n",
    "    timedelta=timedelta,\n",
    "    start_with_input=True,\n",
    ")\n",
    "predictions_ds = model.data_to_xarray(predictions, times=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7lhecHtbo82"
   },
   "source": [
    "## Compare forecast to ERA5\n",
    "\n",
    "See [WeatherBench2](https://sites.research.google/weatherbench/) for more comprehensive evaluations and archived NeuralGCM forecasts.\n",
    "\n",
    "Can stay like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-GG0YrV7cMG",
    "outputId": "5438e4b5-aa35-495e-c2b5-7f43494dcb47"
   },
   "outputs": [],
   "source": [
    "# Selecting ERA5 targets from exactly the same time slice\n",
    "target_trajectory = model.inputs_from_xarray(\n",
    "    eval_era5\n",
    "    .thin(time=(inner_steps // data_inner_steps))\n",
    "    .isel(time=slice(outer_steps))\n",
    ")\n",
    "target_data_ds = model.data_to_xarray(target_trajectory, times=times)\n",
    "\n",
    "combined_ds = xarray.concat([target_data_ds, predictions_ds], 'model')\n",
    "combined_ds.coords['model'] = ['ERA5', 'NeuralGCM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "EUoubIO67uTW",
    "outputId": "f2acc749-a9fb-4cab-a10a-b89e2d016791"
   },
   "outputs": [],
   "source": [
    "# Visualize ERA5 vs NeuralGCM trajectories\n",
    "combined_ds.specific_humidity.sel(level=850).plot(\n",
    "    x='longitude', y='latitude', row='time', col='model', robust=True, aspect=2, size=2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96H8CLDo3Rzx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tkGJv8VEgFa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML-climate",
   "language": "python",
   "name": "ml-climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
