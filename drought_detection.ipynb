{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldOn_BV2GwKP",
    "outputId": "865ec8f2-4e19-4602-a406-b49e5c267e75",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.10\n",
      "Requirement already satisfied: matplotlib in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: cartopy in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (2.0.0)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (3.9.3)\n",
      "Requirement already satisfied: shapely>=1.8 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=21 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (24.1)\n",
      "Requirement already satisfied: pyshp>=2.3 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (2.3.1)\n",
      "Requirement already satisfied: pyproj>=3.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (3.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pyproj>=3.3.1->cartopy) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.16.0)\n",
      "Requirement already satisfied: h5netcdf in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (1.4.1)\n",
      "Requirement already satisfied: h5py in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from h5netcdf) (3.12.1)\n",
      "Requirement already satisfied: packaging in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from h5netcdf) (24.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from h5py->h5netcdf) (2.0.0)\n",
      "Requirement already satisfied: xarray in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: netCDF4 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: h5netcdf in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (1.4.1)\n",
      "Requirement already satisfied: dask in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (2024.12.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from xarray) (2.0.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from xarray) (24.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from xarray) (2.2.3)\n",
      "Requirement already satisfied: cftime in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from netCDF4) (2024.8.30)\n",
      "Requirement already satisfied: h5py in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from h5netcdf) (3.12.1)\n",
      "Requirement already satisfied: click>=8.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from dask) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from dask) (2024.10.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from dask) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2024.2)\n",
      "Requirement already satisfied: locket in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# if necessary, install NeuralGCM and dependencies\n",
    "!python --version\n",
    "!pip install -q -U neuralgcm dinosaur-dycore gcsfs\n",
    "!pip install matplotlib\n",
    "!pip install cartopy\n",
    "!pip install h5netcdf\n",
    "!pip install --upgrade xarray netCDF4 h5netcdf dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wTapB9c0AWMJ"
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "gin.enter_interactive_mode()\n",
    "import gcsfs\n",
    "import jax\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xarray\n",
    "\n",
    "from dinosaur import horizontal_interpolation\n",
    "from dinosaur import spherical_harmonic\n",
    "from dinosaur import xarray_utils\n",
    "import neuralgcm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 34MB\n",
      "Dimensions:                              (time: 64, lat: 64, lon: 128)\n",
      "Coordinates:\n",
      "  * time                                 (time) int64 512B 0 1 2 3 ... 61 62 63\n",
      "  * lat                                  (lat) float64 512B -90.0 ... 90.0\n",
      "  * lon                                  (lon) float64 1kB -180.0 ... 180.0\n",
      "Data variables:\n",
      "    temperature                          (time, lat, lon) float64 4MB 0.4836 ...\n",
      "    geopotential                         (time, lat, lon) float64 4MB 0.3903 ...\n",
      "    specific_cloud_liquid_water_content  (time, lat, lon) float64 4MB 0.6764 ...\n",
      "    u_component_of_wind                  (time, lat, lon) float64 4MB 0.5183 ...\n",
      "    specific_cloud_ice_water_content     (time, lat, lon) float64 4MB 0.03711...\n",
      "    P_minus_E_cumulative                 (time, lat, lon) float64 4MB 0.8406 ...\n",
      "    specific_humidity                    (time, lat, lon) float64 4MB 0.6161 ...\n",
      "    v_component_of_wind                  (time, lat, lon) float64 4MB 0.08324...\n",
      "Minimal dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create a simple dataset\n",
    "\n",
    "# Define dimensions for a ~500 MB dataset\n",
    "\n",
    "# Example dimensions\n",
    "time_dim = 64\n",
    "lat_dim = 64\n",
    "lon_dim = 128\n",
    "\n",
    "# Constructing a dataset similar to `test_data` with your variables\n",
    "converted_data = xr.Dataset(\n",
    "    {\n",
    "        \"temperature\": ((\"time\", \"lat\", \"lon\"), np.random.rand(time_dim, lat_dim, lon_dim)),\n",
    "        \"geopotential\": ((\"time\", \"lat\", \"lon\"), np.random.rand(time_dim, lat_dim, lon_dim)),\n",
    "        \"specific_cloud_liquid_water_content\": ((\"time\", \"lat\", \"lon\"), np.random.rand(time_dim, lat_dim, lon_dim)),\n",
    "        \"u_component_of_wind\": ((\"time\", \"lat\", \"lon\"), np.random.rand(time_dim, lat_dim, lon_dim)),\n",
    "        \"specific_cloud_ice_water_content\": ((\"time\", \"lat\", \"lon\"), np.random.rand(time_dim, lat_dim, lon_dim)),\n",
    "        \"P_minus_E_cumulative\": ((\"time\", \"lat\", \"lon\"), np.random.rand(time_dim, lat_dim, lon_dim)),\n",
    "        \"specific_humidity\": ((\"time\", \"lat\", \"lon\"), np.random.rand(time_dim, lat_dim, lon_dim)),\n",
    "        \"v_component_of_wind\": ((\"time\", \"lat\", \"lon\"), np.random.rand(time_dim, lat_dim, lon_dim)),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": np.arange(time_dim),\n",
    "        \"lat\": np.linspace(-90, 90, lat_dim),\n",
    "        \"lon\": np.linspace(-180, 180, lon_dim),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the dataset\n",
    "print(converted_data)\n",
    "\n",
    "converted_data.to_netcdf(\"test_minimal.nc\")\n",
    "print(\"Minimal dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_web7Ad1gunN"
   },
   "source": [
    "## Background and Motivation\n",
    "\n",
    "<span style=\"color:blue\">Todo later</span> Rewrite Introduction, add any relevant thins into here\n",
    "Droughts are natural disasters that are getting worse every year and therefore affecting millions of people by increasing the risk of malnutrition, diseases, wildfires, or forced migration due to droughts. Developing early warning systems and timely interventions is crucial to mitigate the economic, social, and environmental impacts of droughts.\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> Add other necessary information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: Add if something is missing\n",
    "Data: \\\n",
    "ERA 5 data \\\n",
    "pretrained NeuralGCM model (intermediate deterministic NeuralGCM 1.4 model) \\\n",
    "SST data \n",
    "\n",
    "Objectives: \\\n",
    "The aim of this project is to make a 30-year roll-out prediction for drought frequency and amplitudes in the region of Spain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drought specific variables\n",
    "\n",
    "Evapotranspiration (not in NeuralGCM) \\\n",
    "Precipitation (not in NeuralGCM) \\\n",
    "Temperature \\\n",
    "Specific_humidity \\\n",
    "Sea Surface Temperature (not in NeuralGCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xarray in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: netCDF4 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from xarray) (24.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from xarray) (2.2.3)\n",
      "Requirement already satisfied: cftime in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from netCDF4) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install xarray netCDF4 numpy\n",
    "\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data to be similar to NeuralGCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5uFP46Obo80"
   },
   "source": [
    "## Load a pre-trained NeuralGCM model\n",
    "\n",
    "```{caution}\n",
    "Trained model weights are licensed for non-commercial use, under the Creative Commons [Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/) license (CC BY-NC-SA 4.0).\n",
    "```\n",
    "\n",
    "Pre-trained model checkpoints from the NeuralGCM paper are [available for download](https://console.cloud.google.com/storage/browser/gresearch/neuralgcm/04_30_2024) on Google Cloud Storage:\n",
    "\n",
    "- Deterministic models:\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_0_7_deg.pkl`\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_1_4_deg.pkl`\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_2_8_deg.pkl`\n",
    "- Stochastic models:\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_stochastic_1_4_deg.pkl`\n",
    "\n",
    "## Need to train it on our own using the inputs from era5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and modify the model configuration string\n",
    "\n",
    "Ensure that the new variables (precipitation, soil moisture, evapotranspiration) are available in your dataset and properly preprocessed. The data should be regridded to match NeuralGCM's native grid and provided in the correct units. Refer to NeuralGCM's data preparation guidelines for detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "model_name = 'neural_gcm_dynamic_forcing_deterministic_2_8_deg.pkl'\n",
    "with gcs.open(f'gs://gresearch/neuralgcm/04_30_2024/{model_name}', 'rb') as f:\n",
    "    ckpt = pickle.load(f)\n",
    "\n",
    "new_inputs_to_units_mapping = {\n",
    "    'u': 'meter / second',\n",
    "    'v': 'meter / second',\n",
    "    't': 'kelvin',\n",
    "    'z': 'm**2 s**-2',\n",
    "    'sim_time': 'dimensionless',\n",
    "    'tracers': {'specific_humidity': 'dimensionless',\n",
    "                'specific_cloud_liquid_water_content': 'dimensionless',\n",
    "                'specific_cloud_ice_water_content': 'dimensionless',\n",
    "    },\n",
    "\n",
    "    'diagnostics': {\n",
    "        'surface_pressure': 'kg / (meter s**2)',\n",
    "        'P_minus_E_cumulative': 'kg / (meter**2)'\n",
    "        # Add new diagnostic variables if any\n",
    "    }\n",
    "}\n",
    "\n",
    "new_model_config_str = '\\n'.join([\n",
    "        ckpt['model_config_str'],\n",
    "        f'DimensionalLearnedPrimitiveToWeatherbenchDecoder.inputs_to_units_mapping = {new_inputs_to_units_mapping}',\n",
    "        'DimensionalLearnedPrimitiveToWeatherbenchDecoder.diagnostics_module = @NodalModelDiagnosticsDecoder',\n",
    "        'StochasticPhysicsParameterizationStep.diagnostics_module = @PrecipitationMinusEvaporationDiagnostics',\n",
    "        'PrecipitationMinusEvaporationDiagnostics.method = \"cumulative\"',\n",
    "        'PrecipitationMinusEvaporationDiagnostics.moisture_species =  (\"specific_humidity\", \"specific_cloud_liquid_water_content\", \"specific_cloud_ice_water_content\")',])\n",
    "\n",
    "ckpt['model_config_str'] = new_model_config_str\n",
    "\n",
    "model = neuralgcm.PressureLevelModel.from_checkpoint(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66ZyTazL6GF7"
   },
   "source": [
    "## Subset the data and compute some variables\n",
    "<span style=\"color:red\">TODO </span> SST still NAN for Spain\n",
    "\n",
    "<span style=\"color:blue\">TODO later </span> Add short info about AMO and SST Anomalies?\n",
    "\n",
    "## <span style =\"color:red\"> Need to change start time and end time\n",
    "Chose data: Start_time = 1940-01-01, End_time = 1989-12-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_bounds = [slice(34, 45), slice(34, 51)] \n",
    "lon_bounds = [slice(-25, 19), slice(-20, 10)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpEb_avqbo80"
   },
   "source": [
    "## Load ERA5 data from GCP/Zarr\n",
    "\n",
    "See {doc}`datasets` for details. Leave this part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_path = 'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
    "full_era5 = xarray.open_zarr(gcs.get_mapper(era5_path), chunks=None)\n",
    "\n",
    "start_time = '2022-06-21'\n",
    "end_time = '2022-8-25'\n",
    "data_inner_steps = 24  # process every 24th hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"></span>Incorporate the processed SST data or derived indices into your drought prediction model as predictors or covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'mean_total_precipitation_rate' (time: 1323648,\n",
      "                                                   latitude: 721,\n",
      "                                                   longitude: 1440)> Size: 5TB\n",
      "[1374264299520 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * latitude   (latitude) float32 3kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n",
      "  * longitude  (longitude) float32 6kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "  * time       (time) datetime64[ns] 11MB 1900-01-01 ... 2050-12-31T23:00:00\n",
      "Attributes:\n",
      "    long_name:   Mean total precipitation rate\n",
      "    short_name:  mtpr\n",
      "    units:       kg m**-2 s**-1\n",
      "100m_u_component_of_wind\n",
      "100m_v_component_of_wind\n",
      "10m_u_component_of_neutral_wind\n",
      "10m_u_component_of_wind\n",
      "10m_v_component_of_neutral_wind\n",
      "10m_v_component_of_wind\n",
      "10m_wind_gust_since_previous_post_processing\n",
      "instantaneous_10m_wind_gust\n",
      "mean_direction_of_wind_waves\n",
      "mean_period_of_wind_waves\n",
      "mean_wave_period_based_on_first_moment_for_wind_waves\n",
      "mean_wave_period_based_on_second_moment_for_wind_waves\n",
      "ocean_surface_stress_equivalent_10m_neutral_wind_direction\n",
      "ocean_surface_stress_equivalent_10m_neutral_wind_speed\n",
      "significant_height_of_combined_wind_waves_and_swell\n",
      "significant_height_of_wind_waves\n",
      "u_component_of_wind\n",
      "v_component_of_wind\n",
      "wave_spectral_directional_width_for_wind_waves\n"
     ]
    }
   ],
   "source": [
    "print(full_era5[\"mean_total_precipitation_rate\"])\n",
    "for i in full_era5:\n",
    "    if \"wind\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span> Add SST anomalies to the input variables of the NeuralGCM model\n",
    "<span style=\"color:green\">TODO</span> Error with too much data -> sliced or maybe on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9Dbth-nDjM5F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dimensions: FrozenMappingWarningOnValuesAccess({'time': 1323648, 'latitude': 721, 'longitude': 1440, 'level': 37})\n",
      "Updated Coordinates: Coordinates:\n",
      "  * latitude   (latitude) float32 3kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n",
      "  * level      (level) int64 296B 1 2 3 5 7 10 20 ... 875 900 925 950 975 1000\n",
      "  * longitude  (longitude) float32 6kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "  * time       (time) datetime64[ns] 11MB 1900-01-01 ... 2050-12-31T23:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Verify the change\n",
    "print(\"Updated Dimensions:\", full_era5.dims)\n",
    "print(\"Updated Coordinates:\", full_era5.coords)\n",
    "\n",
    "# Step 1: Subset the region and time range\n",
    "\n",
    "#time_bounds = slice('2000-01-01' ,'2020-12-31')\n",
    "lat_bounds = slice(51, 34)  # Latitude bounds (51°N to 34°N)\n",
    "lon_bounds = slice(-20, 10)  # Longitude bounds (-20°W to 10°E)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivRFAQnt6KKF"
   },
   "source": [
    "Regrid to NeuralGCM's native resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_era5 = (\n",
    "    full_era5\n",
    "    [model.input_variables + model.forcing_variables]\n",
    "    .pipe(\n",
    "        xarray_utils.selective_temporal_shift,\n",
    "        variables=model.forcing_variables,\n",
    "        time_shift='24 hours',\n",
    "    )\n",
    "    .sel(time=slice(start_time, end_time, data_inner_steps))\n",
    "    .compute()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "era5_grid = spherical_harmonic.Grid(\n",
    "    latitude_nodes=full_era5.sizes['latitude'],\n",
    "    longitude_nodes=full_era5.sizes['longitude'],\n",
    "    latitude_spacing=xarray_utils.infer_latitude_spacing(full_era5.latitude),\n",
    "    longitude_offset=xarray_utils.infer_longitude_offset(full_era5.longitude),\n",
    ")\n",
    "\n",
    "regridder = horizontal_interpolation.ConservativeRegridder(\n",
    "    era5_grid, model.data_coords.horizontal, skipna=True\n",
    ")\n",
    "    \n",
    "eval_era5 = xarray_utils.regrid(sliced_era5, regridder)\n",
    "eval_era5 = xarray_utils.fill_nan_with_nearest(eval_era5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span>:Ensure that the combined dataset adheres to NeuralGCM’s expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 545MB\n",
      "Dimensions:                              (time: 64, level: 37, longitude: 128,\n",
      "                                          latitude: 64)\n",
      "Coordinates:\n",
      "  * longitude                            (longitude) float64 1kB 0.0 ... 357.2\n",
      "  * latitude                             (latitude) float64 512B -87.86 ... 8...\n",
      "  * level                                (level) int64 296B 1 2 3 ... 975 1000\n",
      "  * time                                 (time) int64 512B 0 24 48 ... 1488 1512\n",
      "Data variables:\n",
      "    specific_cloud_liquid_water_content  (time, level, longitude, latitude) float32 78MB ...\n",
      "    specific_cloud_ice_water_content     (time, level, longitude, latitude) float32 78MB ...\n",
      "    v_component_of_wind                  (time, level, longitude, latitude) float32 78MB ...\n",
      "    geopotential                         (time, level, longitude, latitude) float32 78MB ...\n",
      "    P_minus_E_cumulative                 (time, longitude, latitude) float32 2MB ...\n",
      "    specific_humidity                    (time, level, longitude, latitude) float32 78MB ...\n",
      "    u_component_of_wind                  (time, level, longitude, latitude) float32 78MB ...\n",
      "    sim_time                             (time) float32 256B ...\n",
      "    temperature                          (time, level, longitude, latitude) float32 78MB ...\n",
      "Attributes:\n",
      "    longitude_wavenumbers:     64\n",
      "    total_wavenumbers:         65\n",
      "    longitude_nodes:           128\n",
      "    latitude_nodes:            64\n",
      "    latitude_spacing:          gauss\n",
      "    longitude_offset:          0.0\n",
      "    radius:                    1.0\n",
      "    spherical_harmonics_impl:  RealSphericalHarmonics\n",
      "    spmd_mesh:                 \n",
      "    centers:                   [1, 2, 3, 5, 7, 10, 20, 30, 50, 70, 100, 125, ...\n",
      "    horizontal_grid_type:      Grid\n",
      "    vertical_grid_type:        PressureCoordinates\n",
      "Processing variable: specific_cloud_liquid_water_content\n",
      "Finished processing variable: specific_cloud_liquid_water_content\n",
      "Processing variable: specific_cloud_ice_water_content\n",
      "Finished processing variable: specific_cloud_ice_water_content\n",
      "Processing variable: v_component_of_wind\n",
      "Finished processing variable: v_component_of_wind\n",
      "Processing variable: geopotential\n",
      "Finished processing variable: geopotential\n",
      "Processing variable: P_minus_E_cumulative\n",
      "Finished processing variable: P_minus_E_cumulative\n",
      "Processing variable: specific_humidity\n",
      "Finished processing variable: specific_humidity\n",
      "Processing variable: u_component_of_wind\n",
      "Finished processing variable: u_component_of_wind\n",
      "Processing variable: sim_time\n",
      "Finished processing variable: sim_time\n",
      "Processing variable: temperature\n",
      "Finished processing variable: temperature\n",
      "All variables processed. Missing values filled with -999 where applicable.\n",
      "All variables processed. Missing values filled with -999.\n",
      "hello\n",
      "marco\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Initialize the model state and define forecast parameters\n",
    "\n",
    "import json\n",
    "\n",
    "# Convert attributes to UTF-8 safe strings\n",
    "def enforce_utf8_attrs(ds):\n",
    "    ds.attrs = json.loads(json.dumps(ds.attrs))\n",
    "    for var in ds.variables:\n",
    "        ds[var].attrs = json.loads(json.dumps(ds[var].attrs))\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "inner_steps = 24  # Save model outputs once every 24 hours\n",
    "outer_steps = 64  # Total of 55 days\n",
    "timedelta = np.timedelta64(1, 'h') * inner_steps\n",
    "times = (np.arange(outer_steps) * inner_steps)  # Time axis in hours\n",
    "\n",
    "# Initialize model state\n",
    "inputs = model.inputs_from_xarray(eval_era5.isel(time=0))\n",
    "input_forcings = model.forcings_from_xarray(eval_era5.isel(time=0))\n",
    "rng_key = jax.random.PRNGKey(42)  # Optional for deterministic models\n",
    "initial_state = model.encode(inputs, input_forcings, rng_key)\n",
    "\n",
    "# Use persistence for forcing variables (SST and sea ice cover)\n",
    "all_forcings = model.forcings_from_xarray(eval_era5.head(time=1))\n",
    "\n",
    "# Step 5: Make forecast\n",
    "final_state, predictions = model.unroll(\n",
    "    initial_state,\n",
    "    all_forcings,\n",
    "    steps=outer_steps,\n",
    "    timedelta=timedelta,\n",
    "    start_with_input=True,\n",
    ")\n",
    "\n",
    "# Convert predictions to xarray dataset\n",
    "predictions_ds = model.data_to_xarray(predictions, times=times)\n",
    "\n",
    "# Step 6: Post-process and save forecast results\n",
    "forecast_dataset = predictions_ds\n",
    "print(forecast_dataset)\n",
    "\n",
    "# Iterate through each variable in the dataset and fill NaN values\n",
    "# Iterate through all variables in the dataset\n",
    "for var in forecast_dataset.data_vars:\n",
    "    print(f\"Processing variable: {var}\")\n",
    "    try:\n",
    "        # Apply fillna only if the variable supports it\n",
    "        forecast_dataset[var] = forecast_dataset[var].fillna(-999)\n",
    "        print(f\"Finished processing variable: {var}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping variable: {var} due to error: {e}\")\n",
    "\n",
    "print(\"All variables processed. Missing values filled with -999 where applicable.\")\n",
    "\n",
    "\n",
    "print(\"All variables processed. Missing values filled with -999.\")\n",
    "\n",
    "print(\"hello\")\n",
    "#forecast_dataset = forecast_dataset.astype(float)\n",
    "\n",
    "print(\"marco\")\n",
    "# Save results to a NetCDF file\n",
    "#forecast_dataset.to_netcdf('neuralgcm_forecast_2022.nc')\n",
    "\n",
    "#forecast_dataset = enforce_utf8_attrs(forecast_dataset)\n",
    "#forecast_dataset.to_netcdf('neuralgcm_forecast_2022.nc')\n",
    "\n",
    "#forecast_dataset.to_zarr('neuralgcm_forecast.zarr')\n",
    "#with h5py.File('neuralgcm_forecast_2022.h5', 'w') as f:\n",
    "#    for var in forecast_dataset.data_vars:\n",
    "#        f.create_dataset(var, data=forecast_dataset[var].values\n",
    "\n",
    "\n",
    "# Define the path to your workspace\n",
    "#workspace_path = \"/pfs/work7/workspace/scratch/kd5572-my_workspace\"\n",
    "#file_name = \"neuralgcm_forecast_2022.nc\"\n",
    "\n",
    "# Full path to save the NetCDF file\n",
    "#save_path = f\"{workspace_path}/{file_name}\"\n",
    "\n",
    "# Save to NetCDF\n",
    "#prediction_ds.to_netcdf(\"test_minimal.nc\")\n",
    "#print(\"Minimal dataset filled with predictions and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Inspecting time step 0\n",
      "<xarray.Dataset> Size: 9MB\n",
      "Dimensions:                              (level: 37, longitude: 128,\n",
      "                                          latitude: 64)\n",
      "Coordinates:\n",
      "  * longitude                            (longitude) float64 1kB 0.0 ... 357.2\n",
      "  * latitude                             (latitude) float64 512B -87.86 ... 8...\n",
      "  * level                                (level) int64 296B 1 2 3 ... 975 1000\n",
      "    time                                 int64 8B 0\n",
      "Data variables:\n",
      "    specific_cloud_liquid_water_content  (level, longitude, latitude) float32 1MB ...\n",
      "    specific_cloud_ice_water_content     (level, longitude, latitude) float32 1MB ...\n",
      "    v_component_of_wind                  (level, longitude, latitude) float32 1MB ...\n",
      "    geopotential                         (level, longitude, latitude) float32 1MB ...\n",
      "    P_minus_E_cumulative                 (longitude, latitude) float32 33kB ...\n",
      "    specific_humidity                    (level, longitude, latitude) float32 1MB ...\n",
      "    u_component_of_wind                  (level, longitude, latitude) float32 1MB ...\n",
      "    sim_time                             float32 4B ...\n",
      "    temperature                          (level, longitude, latitude) float32 1MB ...\n",
      "Attributes:\n",
      "    longitude_wavenumbers:     64\n",
      "    total_wavenumbers:         65\n",
      "    longitude_nodes:           128\n",
      "    latitude_nodes:            64\n",
      "    latitude_spacing:          gauss\n",
      "    longitude_offset:          0.0\n",
      "    radius:                    1.0\n",
      "    spherical_harmonics_impl:  RealSphericalHarmonics\n",
      "    spmd_mesh:                 \n",
      "    centers:                   [1, 2, 3, 5, 7, 10, 20, 30, 50, 70, 100, 125, ...\n",
      "    horizontal_grid_type:      Grid\n",
      "    vertical_grid_type:        PressureCoordinates\n",
      "Testing variable: specific_cloud_liquid_water_content\n",
      "Dataset loaded successfully without specific_cloud_liquid_water_content.\n",
      "Testing variable: specific_cloud_ice_water_content\n",
      "Dataset loaded successfully without specific_cloud_ice_water_content.\n",
      "Testing variable: v_component_of_wind\n",
      "Dataset loaded successfully without v_component_of_wind.\n",
      "Testing variable: geopotential\n",
      "Dataset loaded successfully without geopotential.\n",
      "Testing variable: P_minus_E_cumulative\n",
      "Dataset loaded successfully without P_minus_E_cumulative.\n",
      "Testing variable: specific_humidity\n",
      "Dataset loaded successfully without specific_humidity.\n",
      "Testing variable: u_component_of_wind\n",
      "Dataset loaded successfully without u_component_of_wind.\n",
      "Testing variable: sim_time\n",
      "Dataset loaded successfully without sim_time.\n",
      "Testing variable: temperature\n",
      "Dataset loaded successfully without temperature.\n",
      "Problematic variables: []\n"
     ]
    }
   ],
   "source": [
    "# Save the NetCDF file\n",
    "forecast_dataset = forecast_dataset.compute()\n",
    "print(\"hi\")\n",
    "\n",
    "print(f\"Inspecting time step 0\")\n",
    "subset = forecast_dataset.isel(time=0)\n",
    "print(subset)\n",
    "\n",
    "problematic_vars = []\n",
    "\n",
    "for var in subset.data_vars:\n",
    "    print(f\"Testing variable: {var}\")\n",
    "    try:\n",
    "        test_subset = subset.drop_vars(var)\n",
    "        test_subset.load()\n",
    "        print(f\"Dataset loaded successfully without {var}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with variable {var}: {e}\")\n",
    "        problematic_vars.append(var)\n",
    "\n",
    "print(f\"Problematic variables: {problematic_vars}\")\n",
    "\n",
    "for var in problematic_vars:\n",
    "    subset[var] = subset[var].fillna(-999)  # Replace NaNs\n",
    "    subset[var].attrs = {}  # Remove attributes\n",
    "\n",
    "forecast_dataset.attrs = {}\n",
    "for var in forecast_dataset.data_vars:\n",
    "    forecast_dataset[var].attrs = {}\n",
    "\n",
    "#forecast_dataset.isel(time=0).to_netcdf(\"time0_cleaned.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\n",
      "u\n",
      "u\n",
      "u\n",
      "u\n",
      "u\n",
      "u\n",
      "u\n",
      "u\n",
      "<class 'xarray.core.dataset.Dataset'>\n",
      "<xarray.Dataset> Size: 545MB\n",
      "Dimensions:                              (time: 64, level: 37, longitude: 128,\n",
      "                                          latitude: 64)\n",
      "Coordinates:\n",
      "  * longitude                            (longitude) float64 1kB 0.0 ... 357.2\n",
      "  * latitude                             (latitude) float64 512B -87.86 ... 8...\n",
      "  * level                                (level) int64 296B 1 2 3 ... 975 1000\n",
      "  * time                                 (time) int64 512B 0 24 48 ... 1488 1512\n",
      "Data variables:\n",
      "    specific_cloud_liquid_water_content  (time, level, longitude, latitude) float32 78MB ...\n",
      "    specific_cloud_ice_water_content     (time, level, longitude, latitude) float32 78MB ...\n",
      "    v_component_of_wind                  (time, level, longitude, latitude) float32 78MB ...\n",
      "    geopotential                         (time, level, longitude, latitude) float32 78MB ...\n",
      "    P_minus_E_cumulative                 (time, longitude, latitude) float32 2MB ...\n",
      "    specific_humidity                    (time, level, longitude, latitude) float32 78MB ...\n",
      "    u_component_of_wind                  (time, level, longitude, latitude) float32 78MB ...\n",
      "    sim_time                             (time) float32 256B ...\n",
      "    temperature                          (time, level, longitude, latitude) float32 78MB ...\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "forecast_dataset.attrs = {}\n",
    "for var in forecast_dataset.data_vars:\n",
    "    print(\"u\")\n",
    "    forecast_dataset[var].attrs = {}\n",
    "# Save\n",
    "\n",
    "loaded_data = forecast_dataset.load()\n",
    "print(type(loaded_data))\n",
    "\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joblib dump successful.\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "try:\n",
    "    dump(loaded_data, 'forecast_dataset_test.joblib')\n",
    "    print(\"Joblib dump successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Joblib error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file (dataset, output_path):\n",
    "    print(\"Starting the process\")\n",
    "    chunk_size_kb = 1\n",
    "    chunk_size_bytes = chunk_size_kb * 1024\n",
    "\n",
    "    #needed to ensure to have at least 1 dimension\n",
    "    min_lat_dim = 3\n",
    "    min_lon_dim = 3\n",
    "    \n",
    "    with xr.Dataset() as ds_out:\n",
    "\n",
    "         for i in range(len(dataset.time)):\n",
    "            #use only one day and some lat and lon (min. 3)\n",
    "            time_step = dataset.isel(time=slice(i, i + 1))   \n",
    "            lat_list = list(range(0, time_step.sizes['latitude'], min_lat_dim))\n",
    "            lon_list = list(range(0, time_step.sizes['longitude'], min_lon_dim))\n",
    "\n",
    "             \n",
    "            for lat_first in lat_list:\n",
    "                for lon_first in lon_list:\n",
    "                    lat_last = min(lat_first+ min_lat_dim, time_step.sizes['latitude'])\n",
    "                    lon_last = min(lon_first + min_lon_dim, time_step.sizes['longitude'])\n",
    "    \n",
    "                    chunk = time_step.isel(latitude=slice(lat_first, lat_last),\n",
    "                                               longitude=slice(lon_first, lon_last))\n",
    "\n",
    "                    if chunk.sizes['latitude'] == 0 or chunk.sizes['longitude'] == 0:\n",
    "                        print(\"Skipping chunk with empty latitude or longitude.\")\n",
    "                        continue\n",
    "\n",
    "                    #reduce data (todo don't discard)\n",
    "                    while chunk.nbytes > chunk_size_bytes:\n",
    "                        chunk = chunk.isel(latitude=slice(0, chunk.sizes['latitude'] // 2),\n",
    "                                               longitude=slice(0, chunk.sizes['longitude'] // 2))\n",
    "                    print(f\"Created chunk with size: {chunk.nbytes}\")\n",
    "    \n",
    "                    mode = 'a' if i > 0 or (lat_first > 0 or lon_first > 0) else 'w'\n",
    "                    chunk.to_netcdf(output_path, mode=mode, engine='netcdf4')\n",
    "    \n",
    "                    print(f\"Chunk successfully written to file\")\n",
    "\n",
    "\n",
    "output_path = \"forecast_dataset_chunked.nc\"\n",
    "\n",
    "save_file(forecast_dataset, output_path)\n",
    "print(\"Saving process finished!\")\n",
    "loaded_data = xr.open_dataset(output_path)\n",
    "print(loaded_data)\n",
    "\n",
    "try:\n",
    "    loaded_data = xr.open_dataset(\"forecast_dataset_chunked.nc\")\n",
    "    #loaded_data = xr.open_dataset(\"/hptc_cluster/oi6277/forecast_dataset_chunked.nc\"\n",
    "    print(\"File is loaded:\")\n",
    "    print(loaded_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any NaN values in the entire dataset\n",
    "nan_exists = forecast_dataset.isnull().any()\n",
    "print(\"NaN exists in dataset:\", nan_exists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'dataset.pkl'\n",
    "\n",
    "# Save the dataset using pickle\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(forecast_dataset, f)\n",
    "\n",
    "print(f\"Dataset successfully saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast_dataframe = forecast_dataset.to_dataframe()\n",
    "#forecast_dataframe.to_csv(\"neural.csv\")\n",
    "#forecast_dataset.to_netcdf(\"neuralgcm_2022_28.nc\", \"w\", compute=True, engine=\"netcdf4\")\n",
    "forecast_dataset.load().to_netcdf('neuralgcm_forecast_2022.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    forecast_dataset.to_netcdf('neuralgcm_forecast_2022.nc')\n",
    "    print(\"File written successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error while writing NetCDF file:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrbru3K1bo81"
   },
   "source": [
    "## Make the forecast\n",
    "\n",
    "See {doc}`trained_models` for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7lhecHtbo82"
   },
   "source": [
    "## Compare forecast to ERA5\n",
    "\n",
    "See [WeatherBench2](https://sites.research.google/weatherbench/) for more comprehensive evaluations and archived NeuralGCM forecasts.\n",
    "\n",
    "Can stay like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-GG0YrV7cMG",
    "outputId": "5438e4b5-aa35-495e-c2b5-7f43494dcb47"
   },
   "outputs": [],
   "source": [
    "# Selecting ERA5 targets from exactly the same time slice\n",
    "target_trajectory = model.inputs_from_xarray(\n",
    "    eval_era5\n",
    "    .thin(time=(inner_steps // data_inner_steps))\n",
    "    .isel(time=slice(outer_steps))\n",
    ")\n",
    "print(\"marco\")\n",
    "target_data_ds = model.data_to_xarray(target_trajectory, times=times)\n",
    "print(\"polo\")\n",
    "combined_ds = xarray.concat([target_data_ds, predictions_ds], 'model')\n",
    "print(\"marco\")\n",
    "combined_ds.coords['model'] = ['ERA5', 'NeuralGCM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "EUoubIO67uTW",
    "outputId": "f2acc749-a9fb-4cab-a10a-b89e2d016791"
   },
   "outputs": [],
   "source": [
    "# Visualize ERA5 vs NeuralGCM trajectories\n",
    "combined_ds.temperature.sel(level=850).plot(\n",
    "    x='longitude', y='latitude', row='time', col='model', robust=True, aspect=2, size=2\n",
    ")\n",
    "\n",
    "print(predictions_ds.P_minus_E_cumulative.dims)\n",
    "print(predictions_ds.P_minus_E_cumulative.coords)\n",
    "\n",
    "\n",
    "\n",
    "predictions_ds.P_minus_E_cumulative.sel(time=24).plot(x='longitude', y='latitude', robust=True, aspect=2, size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plot\n",
    "import cartopy.crs as ccrs\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()}, figsize=(10, 5))\n",
    "\n",
    "# Plot the data\n",
    "predictions_ds.P_minus_E_cumulative.sel(time=24).plot(\n",
    "    x='longitude',\n",
    "    y='latitude',\n",
    "    robust=True,\n",
    "    ax=ax,\n",
    "    transform=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "# Add cartographic features\n",
    "ax.coastlines()\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cartopy.feature.LAND, edgecolor='black', facecolor='lightgray', alpha=0.5)\n",
    "\n",
    "# Customize the title and labels\n",
    "ax.set_title(\"Cumulative P minus E at time=24\", fontsize=12)\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==2.0\n",
    "!pip install numba\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ersst_path = \"./data/tos_Omon_GISS-E2-1-G_historical_r1i1p5f1_gn_200101-201412.nc\"\n",
    "ersst_data = xr.open_dataset(ersst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ds['temperature'].sel(\n",
    "    time=slice(outer_steps),\n",
    "    latitude=lat_bounds,\n",
    "    longitude=lon_bounds\n",
    ").plot(x='longitude', y='latitude', row='time', col='model', robust=True, aspect=2, size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST from NeuralGCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ds[sea_surface_temperature].plot(x='longitude', y='latitude', aspect=2, size=2.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import necessary libraries\n",
    "!pip install dask\n",
    "import xarray as xr\n",
    "import dask\n",
    "\n",
    "lat_bounds = slice(34, 51)  # Latitude bounds (51°N to 34°N)\n",
    "lon_bounds = slice(-20, 10)  # Longitude bounds (-20°W to 10°E)\n",
    "\n",
    "# Extract SST with lazy loading\n",
    "sst = predictions_ds['temperature'].sel(\n",
    "    time=slice(outer_steps),\n",
    "    latitude=lat_bounds,\n",
    "    longitude=lon_bounds)\n",
    "print(\"1 done - SST extracted\")\n",
    "\n",
    "# Align time indexing for consistent processing\n",
    "sst['time'] = sst.indexes['time']\n",
    "print(sst)\n",
    "print(\"done\")\n",
    "\n",
    "# Calculate climatological mean SST\n",
    "climatological_mean_sst = sst.mean(dim='time').persist()  # Persist in memory for repeated use\n",
    "print(\"2 done - Climatological mean computed\")\n",
    "print(climatological_mean_sst)\n",
    "# Compute SST anomalies\n",
    "sst_anomalies = (sst - climatological_mean_sst).persist()  # Persist anomalies for further analysis\n",
    "print(\"3 done - SST anomalies computed\")\n",
    "print(sst_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "print(climatological_mean_sst.isel(level=0).values)\n",
    "print(climatological_mean_sst.isel(level=-1))\n",
    "\n",
    "print(climatological_mean_sst['level'])\n",
    "# Subset the data\n",
    "subset_sst = climatological_mean_sst.sel(latitude=lat_bounds, longitude=lon_bounds)\n",
    "\n",
    "# Example for one time step\n",
    "plt.figure()\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "#ax.set_global()\n",
    "ax.coastlines()\n",
    "sst_plot = ax.imshow(\n",
    "    subset_sst.isel(level=-1),\n",
    "    extent=[\n",
    "        subset_sst['longitude'].min(),\n",
    "        subset_sst['longitude'].max(),\n",
    "        subset_sst['latitude'].min(),\n",
    "        subset_sst['latitude'].max(),\n",
    "    ],\n",
    "    transform=ccrs.PlateCarree(), cmap='coolwarm', origin='upper'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plt.colorbar(sst_plot, ax=ax, orientation='horizontal', label='SST Anomalies (°C)')\n",
    "plt.title('Sea Surface Temperature Anomalies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST Pattern  \n",
    "<span style=\"color:green\"></span> Compute anomalies, trends, or indices such as the Atlantic Multidecadal Oscillation (AMO) to understand SST variations over time. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data\n",
    "lat_bounds = [slice(34, 45), slice(34, 51)] \n",
    "lon_bounds = [slice(-25, 19), slice(-20, 10)]  \n",
    "start_time = '2005-01-01'\n",
    "end_time = '2022-06-16'\n",
    "sst_subset = ersst_data['tos'].sel(time=slice(start_time, end_time),\n",
    "                                   lat=lat_bounds[0],\n",
    "                                   lon=lon_bounds[0])\n",
    "print(sst_subset.values)\n",
    "\n",
    "# Calculate the climatology (mean over the period)\n",
    "sst_subset['time'] = sst_subset.indexes['time']\n",
    "\n",
    "climatology = sst_subset.mean(dim='time') \n",
    "\n",
    "print(climatology)\n",
    "print(climatology.dims)\n",
    "print(climatology.shape)\n",
    "# Compute SST anomalies\n",
    "sst_anomalies = sst_subset - climatology\n",
    "\n",
    "sst_anomalies.to_netcdf('sst_anomalies.nc')\n",
    "\n",
    "# Calculate the AMO index (example)\n",
    "amo_index = sst_anomalies.mean(dim=['lat', 'lon'])\n",
    "\n",
    "print(amo_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = [-24, 26, 35, 45] \n",
    "\n",
    "plt.figure()\n",
    "ax = plt.axes(projection=cartopy.crs.PlateCarree(central_longitude=0))\n",
    "ax.set_global()\n",
    "ax.set_extent(region, crs=cartopy.crs.PlateCarree())\n",
    "ax.gridlines(linestyle='--',color='gray')\n",
    "ax.coastlines()\n",
    "\n",
    "temp_cartopy = ax.pcolormesh(\n",
    "    sst_subset['lon'], \n",
    "    sst_subset['lat'], \n",
    "    climatology, \n",
    "    transform=cartopy.crs.PlateCarree(), \n",
    "    shading='auto', \n",
    "    cmap='bwr'\n",
    ")\n",
    "colorbar = plt.colorbar(temp_cartopy, ax=ax, orientation='horizontal', label='Mean Temperature')\n",
    "colorbar.set_label(\"°C\",size=12,rotation=0)\n",
    "plt.title(\"Mean Sea Surface Temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sst_anomalies.dims)\n",
    "print(sst_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.axes(projection=cartopy.crs.PlateCarree(central_longitude=0))\n",
    "ax.set_global()\n",
    "ax.set_extent(region, crs=cartopy.crs.PlateCarree())\n",
    "ax.gridlines(linestyle='--',color='gray')\n",
    "ax.coastlines()\n",
    "\n",
    "temp_cartopy = ax.pcolormesh(sst_subset['lon'], sst_subset['lat'], sst_anomalies[0,:,:], transform=cartopy.crs.PlateCarree(), shading='auto',cmap='bwr')\n",
    "colorbar = plt.colorbar(temp_cartopy, ax=ax, orientation='horizontal', label='SST Anomalies')\n",
    "colorbar.set_label(\"°C\",size=12,rotation=0)\n",
    "plt.title(\"SST Anomalies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(amo_index, label=\"AMO INDEX\", color=\"b\")\n",
    "plt.axhline(0, color=\"k\", linestyle=\"--\", linewidth=0.8, label=\"Zero Anomaly\")\n",
    "plt.title(\"Atlantic Multidecadal Oscillation (AMO) Index\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"AMO Index\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE SST\n",
    "true values from file and predicted_values from neuralgcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((true_values - predicted_values) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate SPEI directly \n",
    "def calculate_spei(predictions_ds, scale=3):\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "\n",
    "    def thornthwaite(temp, lat):\n",
    "        \"\"\"\n",
    "        Thornthwaite method to estimate potential evapotranspiration (PET).\n",
    "        \"\"\"\n",
    "        temp_celsius = temp - 273.15  # Convert from Kelvin to Celsius\n",
    "        if temp_celsius > 0:\n",
    "            I = (temp_celsius / 5) ** 1.514\n",
    "            a = (6.75e-7 * I ** 3) - (7.71e-5 * I ** 2) + (1.792e-2 * I) + 0.49239\n",
    "            PET = 16 * ((10 * temp_celsius / I) ** a)  # Simplified Thornthwaite formula\n",
    "        else:\n",
    "            PET = 0  # PET is zero if temperature is below or equal to zero\n",
    "    \n",
    "        return PET\n",
    "\n",
    "\n",
    "\n",
    "    def compute_spei(D, scale=3):\n",
    "        \"\"\"\n",
    "        Compute Standardized Precipitation Evapotranspiration Index (SPEI) at a given scale.\n",
    "        \"\"\"\n",
    "        rolling_mean = D.rolling(time=scale, center=False).mean()\n",
    "        rolling_std = D.rolling(time=scale, center=False).std()\n",
    "        spei = (rolling_mean - rolling_mean.mean(dim=\"time\")) / rolling_std\n",
    "        print(\"2\")\n",
    "        return spei\n",
    "\n",
    "    # Step 1: Calculate PET\n",
    "    latitude = predictions_ds['latitude']\n",
    "    temperature = predictions_ds['temperature'].sel(level=1000)  # Near-surface temperature\n",
    "    latitude_value = latitude.mean().item()  # Simplified to one value for now\n",
    "    PET = xr.apply_ufunc(thornthwaite, temperature, latitude_value, vectorize=True)\n",
    "\n",
    "# Calculate D\n",
    "    P_minus_E = predictions_ds['P_minus_E_cumulative']\n",
    "    D = P_minus_E - PET\n",
    "# Compute SPEI\n",
    "    SPEI = compute_spei(D, scale=scale)\n",
    "    print(\"3\")\n",
    "    return SPEI\n",
    "\n",
    "SPEI_result = calculate_spei(predictions_ds)\n",
    "print(SPEI_result)\n",
    "spei_avg = SPEI_result.mean(dim=['latitude', 'longitude'])\n",
    "time = spei_avg['time']\n",
    "spei_values = spei_avg.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define the starting date and daily interval\n",
    "start_date = '2022-06-21'  # Replace with your actual start date\n",
    "time = pd.date_range(start=start_date, periods=len(spei_values), freq='D')  # Daily frequency\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time, spei_values, label='SPEI (Spatial Average)', color='blue')\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.8, label='Neutral')  # Reference line\n",
    "plt.axhline(-1, color='orange', linestyle='--', label='Moderate Drought')\n",
    "plt.axhline(-2, color='red', linestyle='--', label='Severe Drought')\n",
    "plt.axhline(1, color='green', linestyle='--', label='Moderate Wet')\n",
    "plt.axhline(2, color='darkgreen', linestyle='--', label='Severe Wet')\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"SPEI Time Series (Spatial Average)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"SPEI Value\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Automatically format date labels on the x-axis\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tkGJv8VEgFa"
   },
   "source": [
    "## Adding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img = Image.open('images/Spain_range_1.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img = Image.open('images/Spain_range_1.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/Spain_range_1.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVCC2pO8eZE0"
   },
   "outputs": [],
   "source": [
    "#import signal\n",
    "#import sys\n",
    "#import numpy as np\n",
    "#import jax\n",
    "#\n",
    "# Parameters for 30-year rollout\n",
    "#years = 30\n",
    "#days_per_year = 365  # Exclude leap years for simplicity\n",
    "#inner_steps = 24  # Save model outputs every 24 hours\n",
    "#hours_per_day = 24\n",
    "outer_steps = (days_per_year * years * hours_per_day) // inner_steps  # Total steps for 30 years\n",
    "timedelta = np.timedelta64(inner_steps, 'h')  # Time interval between model outputs\n",
    "times = np.arange(outer_steps) * inner_steps  # Time axis in hours\n",
    "\n",
    "# Placeholder model and data (replace with actual implementations)\n",
    "# model = ...\n",
    "# eval_era5 = ...\n",
    "\n",
    "class GracefulExit:\n",
    "    \"\"\"Handles graceful exit and file closing.\"\"\"\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.is_running = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        print(\"Shutting down gracefully...\")\n",
    "        if not self.file.closed:\n",
    "            self.file.close()\n",
    "        self.is_running = False\n",
    "\n",
    "# Signal handler to trigger cleanup\n",
    "def handle_signal(signum, frame):\n",
    "    global graceful_exit_context\n",
    "    graceful_exit_context.cleanup()\n",
    "\n",
    "# Register signal handlers\n",
    "signal.signal(signal.SIGINT, handle_signal)\n",
    "signal.signal(signal.SIGTERM, handle_signal)\n",
    "\n",
    "output_file = \"30_year_rollout_predictions.nc\"\n",
    "\n",
    "with open(output_file, \"w\") as nc_file:\n",
    "    with GracefulExit(nc_file) as graceful_exit_context:\n",
    "        try:\n",
    "            # Initialize model state\n",
    "            print(\"Initializing model state...\")\n",
    "            inputs = model.inputs_from_xarray(eval_era5.isel(time=0))\n",
    "            input_forcings = model.forcings_from_xarray(eval_era5.isel(time=0))\n",
    "            rng_key = jax.random.key(42)  # Optional for deterministic models\n",
    "            initial_state = model.encode(inputs, input_forcings, rng_key)\n",
    "\n",
    "            # Use persistence for forcing variables (e.g., SST and sea ice cover)\n",
    "            print(\"Using persistent forcing variables...\")\n",
    "            all_forcings = model.forcings_from_xarray(eval_era5.head(time=1))\n",
    "\n",
    "            # Make forecast\n",
    "            print(f\"Starting 30-year rollout with {outer_steps} steps...\")\n",
    "            final_state, predictions = model.unroll(\n",
    "                initial_state,\n",
    "                all_forcings,\n",
    "                steps=outer_steps,\n",
    "                timedelta=timedelta,\n",
    "                start_with_input=True,\n",
    "            )\n",
    "\n",
    "            # Convert predictions to xarray dataset\n",
    "            print(\"Converting predictions to xarray.Dataset...\")\n",
    "            #print(predictions)\n",
    "            predictions_ds = model.data_to_xarray(predictions, times=times)\n",
    "            print(predictions_ds)\n",
    "            # Save results to a NetCDF file\n",
    "            print(\"Applying chunking to the dataset...\")\n",
    "            #chunked_ds = predictions_ds.chunk({'time': 100})  # Adjust chunk size as needed\n",
    "            #chunked_ds.to_netcdf(output_file, mode='w', engine='netcdf4')\n",
    "\n",
    "\n",
    "            # Final message\n",
    "            print(\"30-year rollout completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            graceful_exit_context.cleanup()\n",
    "        finally:\n",
    "            print(\"Execution stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neuralgcm",
   "language": "python",
   "name": "neuralgcm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
