{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldOn_BV2GwKP",
    "outputId": "865ec8f2-4e19-4602-a406-b49e5c267e75",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.10\n",
      "Requirement already satisfied: matplotlib in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: cartopy in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (2.0.0)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (3.9.3)\n",
      "Requirement already satisfied: shapely>=1.8 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=21 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (24.1)\n",
      "Requirement already satisfied: pyshp>=2.3 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (2.3.1)\n",
      "Requirement already satisfied: pyproj>=3.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from cartopy) (3.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from matplotlib>=3.6->cartopy) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pyproj>=3.3.1->cartopy) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# if necessary, install NeuralGCM and dependencies\n",
    "!python --version\n",
    "!pip install -q -U neuralgcm dinosaur-dycore gcsfs\n",
    "!pip install matplotlib\n",
    "!pip install cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_web7Ad1gunN"
   },
   "source": [
    "## Background and Motivation\n",
    "\n",
    "<span style=\"color:blue\">Todo later</span> Rewrite Introduction, add any relevant thins into here\n",
    "Droughts are natural disasters that are getting worse every year and therefore affecting millions of people by increasing the risk of malnutrition, diseases, wildfires, or forced migration due to droughts. Developing early warning systems and timely interventions is crucial to mitigate the economic, social, and environmental impacts of droughts.\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> Add other necessary information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: Add if something is missing\n",
    "Data: \\\n",
    "ERA 5 data \\\n",
    "pretrained NeuralGCM model (intermediate deterministic NeuralGCM 1.4 model) \\\n",
    "SST data \n",
    "\n",
    "Objectives: \\\n",
    "The aim of this project is to make a 30-year roll-out prediction for drought frequency and amplitudes in the region of Spain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drought specific variables\n",
    "\n",
    "Evapotranspiration (not in NeuralGCM) \\\n",
    "Precipitation (not in NeuralGCM) \\\n",
    "Temperature \\\n",
    "Specific_humidity \\\n",
    "Sea Surface Temperature (not in NeuralGCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wTapB9c0AWMJ"
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "gin.enter_interactive_mode()\n",
    "import gcsfs\n",
    "import jax\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xarray\n",
    "\n",
    "from dinosaur import horizontal_interpolation\n",
    "from dinosaur import spherical_harmonic\n",
    "from dinosaur import xarray_utils\n",
    "import neuralgcm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xarray in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: netCDF4 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from xarray) (24.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from xarray) (2.2.3)\n",
      "Requirement already satisfied: cftime in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from netCDF4) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install xarray netCDF4 numpy\n",
    "\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data to be similar to NeuralGCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5uFP46Obo80"
   },
   "source": [
    "## Load a pre-trained NeuralGCM model\n",
    "\n",
    "```{caution}\n",
    "Trained model weights are licensed for non-commercial use, under the Creative Commons [Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/) license (CC BY-NC-SA 4.0).\n",
    "```\n",
    "\n",
    "Pre-trained model checkpoints from the NeuralGCM paper are [available for download](https://console.cloud.google.com/storage/browser/gresearch/neuralgcm/04_30_2024) on Google Cloud Storage:\n",
    "\n",
    "- Deterministic models:\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_0_7_deg.pkl`\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_1_4_deg.pkl`\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_deterministic_2_8_deg.pkl`\n",
    "- Stochastic models:\n",
    "    - `gs://gresearch/neuralgcm/04_30_2024/neural_gcm_dynamic_forcing_stochastic_1_4_deg.pkl`\n",
    "\n",
    "## Need to train it on our own using the inputs from era5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and modify the model configuration string\n",
    "\n",
    "Ensure that the new variables (precipitation, soil moisture, evapotranspiration) are available in your dataset and properly preprocessed. The data should be regridded to match NeuralGCM's native grid and provided in the correct units. Refer to NeuralGCM's data preparation guidelines for detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "model_name = 'neural_gcm_dynamic_forcing_deterministic_2_8_deg.pkl'\n",
    "with gcs.open(f'gs://gresearch/neuralgcm/04_30_2024/{model_name}', 'rb') as f:\n",
    "    ckpt = pickle.load(f)\n",
    "\n",
    "new_inputs_to_units_mapping = {\n",
    "    'u': 'meter / second',\n",
    "    'v': 'meter / second',\n",
    "    't': 'kelvin',\n",
    "    'z': 'm**2 s**-2',\n",
    "    'sim_time': 'dimensionless',\n",
    "    'tracers': {'specific_humidity': 'dimensionless',\n",
    "                'specific_cloud_liquid_water_content': 'dimensionless',\n",
    "                'specific_cloud_ice_water_content': 'dimensionless',\n",
    "    },\n",
    "\n",
    "    'diagnostics': {\n",
    "        'surface_pressure': 'kg / (meter s**2)',\n",
    "        'sst_anomalies': 'kelvin',  # SST anomalies in Kelvin\n",
    "        'P_minus_E_cumulative': 'kg / (meter**2)'\n",
    "        # Add new diagnostic variables if any\n",
    "    }\n",
    "}\n",
    "\n",
    "new_model_config_str = '\\n'.join([\n",
    "        ckpt['model_config_str'],\n",
    "        f'DimensionalLearnedPrimitiveToWeatherbenchDecoder.inputs_to_units_mapping = {new_inputs_to_units_mapping}',\n",
    "        'DimensionalLearnedPrimitiveToWeatherbenchDecoder.diagnostics_module = @NodalModelDiagnosticsDecoder',\n",
    "        'StochasticPhysicsParameterizationStep.diagnostics_module = @PrecipitationMinusEvaporationDiagnostics',\n",
    "        'PrecipitationMinusEvaporationDiagnostics.method = \"cumulative\"',\n",
    "        'PrecipitationMinusEvaporationDiagnostics.moisture_species =  (\"specific_humidity\", \"specific_cloud_liquid_water_content\", \"specific_cloud_ice_water_content\")',])\n",
    "\n",
    "ckpt['model_config_str'] = new_model_config_str\n",
    "\n",
    "model = neuralgcm.PressureLevelModel.from_checkpoint(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66ZyTazL6GF7"
   },
   "source": [
    "## Subset the data and compute some variables\n",
    "<span style=\"color:red\">TODO </span> SST still NAN for Spain\n",
    "\n",
    "<span style=\"color:blue\">TODO later </span> Add short info about AMO and SST Anomalies?\n",
    "\n",
    "## <span style =\"color:red\"> Need to change start time and end time\n",
    "Chose data: Start_time = 1940-01-01, End_time = 1989-12-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_bounds = [slice(34, 45), slice(34, 51)] \n",
    "lon_bounds = [slice(-25, 19), slice(-20, 10)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpEb_avqbo80"
   },
   "source": [
    "## Load ERA5 data from GCP/Zarr\n",
    "\n",
    "See {doc}`datasets` for details. Leave this part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_path = 'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
    "full_era5 = xarray.open_zarr(gcs.get_mapper(era5_path), chunks=None)\n",
    "\n",
    "start_time = '2022-06-01'\n",
    "end_time = '2022-06-16'\n",
    "data_inner_steps = 24  # process every 24th hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"></span>Incorporate the processed SST data or derived indices into your drought prediction model as predictors or covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'mean_total_precipitation_rate' (time: 1323648,\n",
      "                                                   latitude: 721,\n",
      "                                                   longitude: 1440)> Size: 5TB\n",
      "[1374264299520 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * latitude   (latitude) float32 3kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n",
      "  * longitude  (longitude) float32 6kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "  * time       (time) datetime64[ns] 11MB 1900-01-01 ... 2050-12-31T23:00:00\n",
      "Attributes:\n",
      "    long_name:   Mean total precipitation rate\n",
      "    short_name:  mtpr\n",
      "    units:       kg m**-2 s**-1\n",
      "100m_u_component_of_wind\n",
      "100m_v_component_of_wind\n",
      "10m_u_component_of_neutral_wind\n",
      "10m_u_component_of_wind\n",
      "10m_v_component_of_neutral_wind\n",
      "10m_v_component_of_wind\n",
      "10m_wind_gust_since_previous_post_processing\n",
      "instantaneous_10m_wind_gust\n",
      "mean_direction_of_wind_waves\n",
      "mean_period_of_wind_waves\n",
      "mean_wave_period_based_on_first_moment_for_wind_waves\n",
      "mean_wave_period_based_on_second_moment_for_wind_waves\n",
      "ocean_surface_stress_equivalent_10m_neutral_wind_direction\n",
      "ocean_surface_stress_equivalent_10m_neutral_wind_speed\n",
      "significant_height_of_combined_wind_waves_and_swell\n",
      "significant_height_of_wind_waves\n",
      "u_component_of_wind\n",
      "v_component_of_wind\n",
      "wave_spectral_directional_width_for_wind_waves\n"
     ]
    }
   ],
   "source": [
    "print(full_era5[\"mean_total_precipitation_rate\"])\n",
    "for i in full_era5:\n",
    "    if \"wind\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span> Add SST anomalies to the input variables of the NeuralGCM model\n",
    "<span style=\"color:green\">TODO</span> Error with too much data -> sliced or maybe on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Dbth-nDjM5F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dimensions: FrozenMappingWarningOnValuesAccess({'time': 1323648, 'latitude': 721, 'longitude': 1440, 'level': 37})\n",
      "Updated Coordinates: Coordinates:\n",
      "  * latitude   (latitude) float32 3kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n",
      "  * level      (level) int64 296B 1 2 3 5 7 10 20 ... 875 900 925 950 975 1000\n",
      "  * longitude  (longitude) float32 6kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "  * time       (time) datetime64[ns] 11MB 1900-01-01 ... 2050-12-31T23:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Verify the change\n",
    "print(\"Updated Dimensions:\", full_era5.dims)\n",
    "print(\"Updated Coordinates:\", full_era5.coords)\n",
    "\n",
    "# Step 1: Subset the region and time range\n",
    "\n",
    "time_bounds = slice('2000-01-01' ,'2020-12-31')\n",
    "lat_bounds = slice(51, 34)  # Latitude bounds (51°N to 34°N)\n",
    "lon_bounds = slice(-20, 10)  # Longitude bounds (-20°W to 10°E)\n",
    "\n",
    "#subset = full_era5.sel(latitude=lat_bounds, longitude=lon_bounds, time=time_bounds)\n",
    "\n",
    "\n",
    "# # Add SST anomalies and other relevant inputs to the dataset\n",
    "# era5_with_sst = xr.Dataset(\n",
    "#     {\n",
    "#         'precipitation': subset.get('mean_total_precipitation_rate', None),\n",
    "#         'temperature': subset.get('temperature', None),\n",
    "#         'specific_humidity': subset.get('specific_humidity', None),\n",
    "#         'sst_anomalies': sst_anomalies,\n",
    "#         'evapotranspiration': subset.get('evaporation', None),\n",
    "#         'soil_moisture': subset.get('volumetric_soil_water_layer_1', None),\n",
    "#         'surface_pressure': subset.get('surface_pressure', None),\n",
    "#     },\n",
    "#     coords={\n",
    "#         'time': subset['time'],\n",
    "#         'latitude': subset['latitude'],\n",
    "#         'longitude': subset['longitude'],\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Display the resulting smaller dataset\n",
    "# print(\"Final dataset dimensions and variables:\")\n",
    "# print(era5_with_sst)\n",
    "\n",
    "# era5_with_sst.to_netcdf(\"era5_with_sst.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivRFAQnt6KKF"
   },
   "source": [
    "Regrid to NeuralGCM's native resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_era5 = (\n",
    "    full_era5\n",
    "    [model.input_variables + model.forcing_variables]\n",
    "    .pipe(\n",
    "        xarray_utils.selective_temporal_shift,\n",
    "        variables=model.forcing_variables,\n",
    "        time_shift='24 hours',\n",
    "    )\n",
    "    .sel(time=slice(start_time, end_time, data_inner_steps))\n",
    "    .compute()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "era5_grid = spherical_harmonic.Grid(\n",
    "    latitude_nodes=full_era5.sizes['latitude'],\n",
    "    longitude_nodes=full_era5.sizes['longitude'],\n",
    "    latitude_spacing=xarray_utils.infer_latitude_spacing(full_era5.latitude),\n",
    "    longitude_offset=xarray_utils.infer_longitude_offset(full_era5.longitude),\n",
    ")\n",
    "\n",
    "regridder = horizontal_interpolation.ConservativeRegridder(\n",
    "    era5_grid, model.data_coords.horizontal, skipna=True\n",
    ")\n",
    "    \n",
    "eval_era5 = xarray_utils.regrid(sliced_era5, regridder)\n",
    "eval_era5 = xarray_utils.fill_nan_with_nearest(eval_era5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">NEW</span>:Ensure that the combined dataset adheres to NeuralGCM’s expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 119MB\n",
      "Dimensions:                              (time: 14, level: 37, longitude: 128,\n",
      "                                          latitude: 64)\n",
      "Coordinates:\n",
      "  * longitude                            (longitude) float64 1kB 0.0 ... 357.2\n",
      "  * latitude                             (latitude) float64 512B -87.86 ... 8...\n",
      "  * level                                (level) int64 296B 1 2 3 ... 975 1000\n",
      "  * time                                 (time) int64 112B 0 24 48 ... 288 312\n",
      "Data variables:\n",
      "    specific_cloud_ice_water_content     (time, level, longitude, latitude) float32 17MB ...\n",
      "    specific_cloud_liquid_water_content  (time, level, longitude, latitude) float32 17MB ...\n",
      "    temperature                          (time, level, longitude, latitude) float32 17MB ...\n",
      "    u_component_of_wind                  (time, level, longitude, latitude) float32 17MB ...\n",
      "    geopotential                         (time, level, longitude, latitude) float32 17MB ...\n",
      "    P_minus_E_cumulative                 (time, longitude, latitude) float32 459kB ...\n",
      "    v_component_of_wind                  (time, level, longitude, latitude) float32 17MB ...\n",
      "    sim_time                             (time) float32 56B ...\n",
      "    specific_humidity                    (time, level, longitude, latitude) float32 17MB ...\n",
      "Attributes:\n",
      "    longitude_wavenumbers:     64\n",
      "    total_wavenumbers:         65\n",
      "    longitude_nodes:           128\n",
      "    latitude_nodes:            64\n",
      "    latitude_spacing:          gauss\n",
      "    longitude_offset:          0.0\n",
      "    radius:                    1.0\n",
      "    spherical_harmonics_impl:  RealSphericalHarmonics\n",
      "    spmd_mesh:                 \n",
      "    centers:                   [1, 2, 3, 5, 7, 10, 20, 30, 50, 70, 100, 125, ...\n",
      "    horizontal_grid_type:      Grid\n",
      "    vertical_grid_type:        PressureCoordinates\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Initialize the model state and define forecast parameters\n",
    "inner_steps = 24  # Save model outputs once every 24 hours\n",
    "outer_steps = 14 * 24 // inner_steps  # Total of 31 days\n",
    "timedelta = np.timedelta64(1, 'h') * inner_steps\n",
    "times = (np.arange(outer_steps) * inner_steps)  # Time axis in hours\n",
    "\n",
    "# Initialize model state\n",
    "inputs = model.inputs_from_xarray(eval_era5.isel(time=0))\n",
    "input_forcings = model.forcings_from_xarray(eval_era5.isel(time=0))\n",
    "rng_key = jax.random.PRNGKey(42)  # Optional for deterministic models\n",
    "initial_state = model.encode(inputs, input_forcings, rng_key)\n",
    "\n",
    "# Use persistence for forcing variables (SST and sea ice cover)\n",
    "all_forcings = model.forcings_from_xarray(eval_era5.head(time=1))\n",
    "\n",
    "# Step 5: Make forecast\n",
    "final_state, predictions = model.unroll(\n",
    "    initial_state,\n",
    "    all_forcings,\n",
    "    steps=outer_steps,\n",
    "    timedelta=timedelta,\n",
    "    start_with_input=True,\n",
    ")\n",
    "\n",
    "# Convert predictions to xarray dataset\n",
    "predictions_ds = model.data_to_xarray(predictions, times=times)\n",
    "\n",
    "# Step 6: Post-process and save forecast results\n",
    "forecast_dataset = predictions_ds\n",
    "print(forecast_dataset)\n",
    "# Save results to a NetCDF file\n",
    "#forecast_dataset.to_netcdf('neuralgcm_forecast_2022.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrbru3K1bo81"
   },
   "source": [
    "## Make the forecast\n",
    "\n",
    "See {doc}`trained_models` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kVCC2pO8eZE0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'days_per_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#import signal\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import sys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#import numpy as np\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#inner_steps = 24  # Save model outputs every 24 hours\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#hours_per_day = 24\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outer_steps \u001b[38;5;241m=\u001b[39m (days_per_year \u001b[38;5;241m*\u001b[39m years \u001b[38;5;241m*\u001b[39m hours_per_day) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m inner_steps  \u001b[38;5;66;03m# Total steps for 30 years\u001b[39;00m\n\u001b[1;32m     12\u001b[0m timedelta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtimedelta64(inner_steps, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Time interval between model outputs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(outer_steps) \u001b[38;5;241m*\u001b[39m inner_steps  \u001b[38;5;66;03m# Time axis in hours\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'days_per_year' is not defined"
     ]
    }
   ],
   "source": [
    "#import signal\n",
    "#import sys\n",
    "#import numpy as np\n",
    "#import jax\n",
    "#\n",
    "# Parameters for 30-year rollout\n",
    "#years = 30\n",
    "#days_per_year = 365  # Exclude leap years for simplicity\n",
    "#inner_steps = 24  # Save model outputs every 24 hours\n",
    "#hours_per_day = 24\n",
    "outer_steps = (days_per_year * years * hours_per_day) // inner_steps  # Total steps for 30 years\n",
    "timedelta = np.timedelta64(inner_steps, 'h')  # Time interval between model outputs\n",
    "times = np.arange(outer_steps) * inner_steps  # Time axis in hours\n",
    "\n",
    "# Placeholder model and data (replace with actual implementations)\n",
    "# model = ...\n",
    "# eval_era5 = ...\n",
    "\n",
    "class GracefulExit:\n",
    "    \"\"\"Handles graceful exit and file closing.\"\"\"\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.is_running = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        print(\"Shutting down gracefully...\")\n",
    "        if not self.file.closed:\n",
    "            self.file.close()\n",
    "        self.is_running = False\n",
    "\n",
    "# Signal handler to trigger cleanup\n",
    "def handle_signal(signum, frame):\n",
    "    global graceful_exit_context\n",
    "    graceful_exit_context.cleanup()\n",
    "\n",
    "# Register signal handlers\n",
    "signal.signal(signal.SIGINT, handle_signal)\n",
    "signal.signal(signal.SIGTERM, handle_signal)\n",
    "\n",
    "output_file = \"30_year_rollout_predictions.nc\"\n",
    "\n",
    "with open(output_file, \"w\") as nc_file:\n",
    "    with GracefulExit(nc_file) as graceful_exit_context:\n",
    "        try:\n",
    "            # Initialize model state\n",
    "            print(\"Initializing model state...\")\n",
    "            inputs = model.inputs_from_xarray(eval_era5.isel(time=0))\n",
    "            input_forcings = model.forcings_from_xarray(eval_era5.isel(time=0))\n",
    "            rng_key = jax.random.key(42)  # Optional for deterministic models\n",
    "            initial_state = model.encode(inputs, input_forcings, rng_key)\n",
    "\n",
    "            # Use persistence for forcing variables (e.g., SST and sea ice cover)\n",
    "            print(\"Using persistent forcing variables...\")\n",
    "            all_forcings = model.forcings_from_xarray(eval_era5.head(time=1))\n",
    "\n",
    "            # Make forecast\n",
    "            print(f\"Starting 30-year rollout with {outer_steps} steps...\")\n",
    "            final_state, predictions = model.unroll(\n",
    "                initial_state,\n",
    "                all_forcings,\n",
    "                steps=outer_steps,\n",
    "                timedelta=timedelta,\n",
    "                start_with_input=True,\n",
    "            )\n",
    "\n",
    "            # Convert predictions to xarray dataset\n",
    "            print(\"Converting predictions to xarray.Dataset...\")\n",
    "            #print(predictions)\n",
    "            predictions_ds = model.data_to_xarray(predictions, times=times)\n",
    "            print(predictions_ds)\n",
    "            # Save results to a NetCDF file\n",
    "            print(\"Applying chunking to the dataset...\")\n",
    "            #chunked_ds = predictions_ds.chunk({'time': 100})  # Adjust chunk size as needed\n",
    "            #chunked_ds.to_netcdf(output_file, mode='w', engine='netcdf4')\n",
    "\n",
    "\n",
    "            # Final message\n",
    "            print(\"30-year rollout completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            graceful_exit_context.cleanup()\n",
    "        finally:\n",
    "            print(\"Execution stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7lhecHtbo82"
   },
   "source": [
    "## Compare forecast to ERA5\n",
    "\n",
    "See [WeatherBench2](https://sites.research.google/weatherbench/) for more comprehensive evaluations and archived NeuralGCM forecasts.\n",
    "\n",
    "Can stay like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-GG0YrV7cMG",
    "outputId": "5438e4b5-aa35-495e-c2b5-7f43494dcb47"
   },
   "outputs": [],
   "source": [
    "# Selecting ERA5 targets from exactly the same time slice\n",
    "target_trajectory = model.inputs_from_xarray(\n",
    "    eval_era5\n",
    "    .thin(time=(inner_steps // data_inner_steps))\n",
    "    .isel(time=slice(outer_steps))\n",
    ")\n",
    "target_data_ds = model.data_to_xarray(target_trajectory, times=times)\n",
    "\n",
    "combined_ds = xarray.concat([target_data_ds, predictions_ds], 'model')\n",
    "combined_ds.coords['model'] = ['ERA5', 'NeuralGCM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "EUoubIO67uTW",
    "outputId": "f2acc749-a9fb-4cab-a10a-b89e2d016791"
   },
   "outputs": [],
   "source": [
    "# Visualize ERA5 vs NeuralGCM trajectories\n",
    "combined_ds.specific_humidity.sel(level=850).plot(\n",
    "    x='longitude', y='latitude', row='time', col='model', robust=True, aspect=2, size=2\n",
    ");\n",
    "\n",
    "combined_ds.P_minus_E_cumulative.sel(level=500).plot(\n",
    "    x='longitude', y='latitude', row='time', col='model', robust=True, aspect=2, size=2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from numba) (0.43.0)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in /home/kit/iti/kd5572/miniconda3/envs/neuralgcm/lib/python3.11/site-packages (from numba) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy==2.0\n",
    "!pip install numba\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ersst_path = \"./data/tos_Omon_GISS-E2-1-G_historical_r1i1p5f1_gn_200101-201412.nc\"\n",
    "ersst_data = xr.open_dataset(ersst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST from NeuralGCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temperature at the lowest pressure level (e.g., 1000 hPa)\n",
    "sst = predictions_ds['temperature'].sel(level=1000)  # Replace 1000 with the correct level if different\n",
    "\n",
    "# Calculate climatological mean SST\n",
    "climatological_mean_sst = sst.mean(dim='time')\n",
    "\n",
    "# Compute SST anomalies\n",
    "sst_anomalies = sst - climatological_mean_sst\n",
    "\n",
    "# Display results to user\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"SST Dataset\", dataframe=sst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Example for one time step\n",
    "plt.figure()\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "ax.coastlines()\n",
    "sst_plot = ax.pcolormesh(\n",
    "    predictions_ds['longitude'], predictions_ds['latitude'], sst_anomalies.isel(time=0),\n",
    "    transform=ccrs.PlateCarree(), cmap='coolwarm', shading='auto'\n",
    ")\n",
    "plt.colorbar(sst_plot, ax=ax, orientation='horizontal', label='SST Anomalies (°C)')\n",
    "plt.title('Sea Surface Temperature Anomalies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST Pattern  \n",
    "<span style=\"color:green\"></span> Compute anomalies, trends, or indices such as the Atlantic Multidecadal Oscillation (AMO) to understand SST variations over time. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "<xarray.DataArray 'tos' (lat: 6, lon: 8)> Size: 192B\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan]], dtype=float32)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 48B 35.0 37.0 39.0 41.0 43.0 45.0\n",
      "  * lon      (lon) float64 64B 1.25 3.75 6.25 8.75 11.25 13.75 16.25 18.75\n",
      "('lat', 'lon')\n",
      "(6, 8)\n",
      "<xarray.DataArray 'tos' (time: 0)> Size: 0B\n",
      "array([], dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) object 0B \n"
     ]
    }
   ],
   "source": [
    "# Subset the data\n",
    "lat_bounds = [slice(34, 45), slice(34, 51)] \n",
    "lon_bounds = [slice(-25, 19), slice(-20, 10)]  \n",
    "sst_subset = ersst_data['tos'].sel(time=slice(start_time, end_time),\n",
    "                                   lat=lat_bounds[0],\n",
    "                                   lon=lon_bounds[0])\n",
    "print(sst_subset.values)\n",
    "\n",
    "# Calculate the climatology (mean over the period)\n",
    "sst_subset['time'] = sst_subset.indexes['time']\n",
    "\n",
    "climatology = sst_subset.mean(dim='time') \n",
    "\n",
    "print(climatology)\n",
    "print(climatology.dims)\n",
    "print(climatology.shape)\n",
    "# Compute SST anomalies\n",
    "sst_anomalies = sst_subset - climatology\n",
    "\n",
    "sst_anomalies.to_netcdf('sst_anomalies.nc')\n",
    "\n",
    "# Calculate the AMO index (example)\n",
    "amo_index = sst_anomalies.mean(dim=['lat', 'lon'])\n",
    "\n",
    "print(amo_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = [-24, 26, 35, 45] \n",
    "\n",
    "plt.figure()\n",
    "ax = plt.axes(projection=cartopy.crs.PlateCarree(central_longitude=0))\n",
    "ax.set_global()\n",
    "ax.set_extent(region, crs=cartopy.crs.PlateCarree())\n",
    "ax.gridlines(linestyle='--',color='gray')\n",
    "ax.coastlines()\n",
    "\n",
    "temp_cartopy = ax.pcolormesh(\n",
    "    sst_subset['lon'], \n",
    "    sst_subset['lat'], \n",
    "    climatology, \n",
    "    transform=cartopy.crs.PlateCarree(), \n",
    "    shading='auto', \n",
    "    cmap='bwr'\n",
    ")\n",
    "colorbar = plt.colorbar(temp_cartopy, ax=ax, orientation='horizontal', label='Mean Temperature')\n",
    "colorbar.set_label(\"°C\",size=12,rotation=0)\n",
    "plt.title(\"Mean Sea Surface Temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sst_anomalies.dims)\n",
    "print(sst_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.axes(projection=cartopy.crs.PlateCarree(central_longitude=0))\n",
    "ax.set_global()\n",
    "ax.set_extent(region, crs=cartopy.crs.PlateCarree())\n",
    "ax.gridlines(linestyle='--',color='gray')\n",
    "ax.coastlines()\n",
    "\n",
    "temp_cartopy = ax.pcolormesh(sst_subset['lon'], sst_subset['lat'], sst_anomalies[0,:,:], transform=cartopy.crs.PlateCarree(), shading='auto',cmap='bwr')\n",
    "colorbar = plt.colorbar(temp_cartopy, ax=ax, orientation='horizontal', label='SST Anomalies')\n",
    "colorbar.set_label(\"°C\",size=12,rotation=0)\n",
    "plt.title(\"SST Anomalies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(amo_index, label=\"AMO INDEX\", color=\"b\")\n",
    "plt.axhline(0, color=\"k\", linestyle=\"--\", linewidth=0.8, label=\"Zero Anomaly\")\n",
    "plt.title(\"Atlantic Multidecadal Oscillation (AMO) Index\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"AMO Index\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE SST\n",
    "true values from file and predicted_values from neuralgcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((true_values - predicted_values) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate SPEI directly\n",
    "def calculate_spei(predictions_ds, scale=3):\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "\n",
    "    def thornthwaite(temp, lat):\n",
    "        \"\"\"\n",
    "        Calculate Potential Evapotranspiration (PET) using Thornthwaite's equation.\n",
    "        Assumes monthly data with latitude-dependent daylight duration.\n",
    "        \"\"\"\n",
    "        I = sum((temp[temp > 0] / 5) ** 1.514)  # Monthly temperature index\n",
    "        a = (6.75e-7 * I ** 3) - (7.71e-5 * I ** 2) + (1.792e-2 * I) + 0.49239\n",
    "        PET = np.zeros_like(temp)\n",
    "        for i, t in enumerate(temp):\n",
    "            if t > 0:\n",
    "                L = 12  # Assume average 12 hours of daylight\n",
    "                PET[i] = (16 * ((10 * t / I) ** a) * (L / 12))\n",
    "        return PET\n",
    "\n",
    "    def compute_spei(D, scale=3):\n",
    "        \"\"\"\n",
    "        Compute Standardized Precipitation Evapotranspiration Index (SPEI) at a given scale.\n",
    "        \"\"\"\n",
    "        rolling_mean = D.rolling(time=scale, center=False).mean()\n",
    "        rolling_std = D.rolling(time=scale, center=False).std()\n",
    "        spei = (rolling_mean - rolling_mean.mean(dim=\"time\")) / rolling_std\n",
    "        return spei\n",
    "\n",
    "    # Step 1: Calculate PET\n",
    "    latitude = predictions_ds['latitude']\n",
    "    temperature = predictions_ds['temperature'].sel(level=1000)  # Near-surface temperature\n",
    "    latitude_value = latitude.mean().item()  # Simplified to one value for now\n",
    "    PET = xr.apply_ufunc(thornthwaite, temperature, latitude_value, vectorize=True)\n",
    "\n",
    "    # Step 2: Calculate D (P - PET)\n",
    "    P_minus_E = predictions_ds['P_minus_E_cumulative']\n",
    "    D = P_minus_E - PET\n",
    "\n",
    "    # Step 3: Compute SPEI\n",
    "    SPEI = compute_spei(D, scale=scale)\n",
    "    return SPEI\n",
    "\n",
    "# Assuming `predictions_ds` is already loaded in memory:\n",
    "SPEI_result = calculate_spei(predictions_ds)\n",
    "\n",
    "# Display SPEI result for inspection\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"SPEI Dataset\", dataframe=SPEI_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def thornthwaite(temp, lat):\n",
    "    \"\"\"Calculate Potential Evapotranspiration (PET) using Thornthwaite's equation.\"\"\"\n",
    "    N = len(temp)\n",
    "    I = sum((temp[temp > 0] / 5) ** 1.514)  # Monthly temperature index\n",
    "    a = (6.75e-7 * I ** 3) - (7.71e-5 * I ** 2) + (1.792e-2 * I) + 0.49239\n",
    "    PET = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if temp[i] > 0:\n",
    "            L = 12  # Assume 12 hours of daylight on average\n",
    "            PET[i] = (16 * ((10 * temp[i] / I) ** a) * (L / 12))\n",
    "    return PET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating PET...\")\n",
    "predictions_ds['PET'] = xr.apply_ufunc(\n",
    "    thornthwaite,\n",
    "    predictions_ds['temperature'],\n",
    "    kwargs=predictions_ds['latitude'],  # Replace 45 with the actual latitude if available\n",
    "    vectorize=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating D (P - PET)...\")\n",
    "predictions_ds['D'] = predictions_ds['P_minus_E_cumulative'] - predictions_ds['PET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spei(D, scale=3):\n",
    "    \"\"\"Compute SPEI at a given scale (e.g., 3-month rolling).\"\"\"\n",
    "    rolling_mean = D.rolling(time=scale, center=False).mean()\n",
    "    rolling_std = D.rolling(time=scale, center=False).std()\n",
    "    spei = (rolling_mean - rolling_mean.mean(dim=\"time\")) / rolling_std\n",
    "    return spei\n",
    "\n",
    "print(\"Calculating SPEI...\")\n",
    "predictions_ds['SPEI'] = compute_spei(predictions_ds['D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96H8CLDo3Rzx"
   },
   "outputs": [],
   "source": [
    "#Plot SPEI\n",
    "def plot_spei(spei, lat=0, lon=0):\n",
    "    \"\"\"Plot SPEI time series for a given location.\"\"\"\n",
    "    spei_point = spei.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    spei_point.plot(label=\"SPEI\")\n",
    "    plt.axhline(0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "    plt.title(f\"SPEI at Location (lat: {lat}, lon: {lon})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"SPEI\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example plot for a specific location (adjust latitude/longitude)\n",
    "print(\"Plotting SPEI for (lat=0, lon=0)...\")\n",
    "plot_spei(predictions_ds['SPEI'], lat=45, lon=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tkGJv8VEgFa"
   },
   "source": [
    "## Adding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img = Image.open('images/Spain_range_1.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img = Image.open('images/Spain_range_1.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/Spain_range_1.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neuralgcm",
   "language": "python",
   "name": "neuralgcm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
